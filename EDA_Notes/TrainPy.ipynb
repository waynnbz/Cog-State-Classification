{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config (to be imported from separate file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category & headers\n",
    "CATEGORY = {'low':0, 'medium':1, 'high':2, 'baseline':3, 'channelized':4, 'surprise':5}\n",
    "REV_CATEGORY = { v:k for k,v in CATEGORY.items()}\n",
    "OUTPUT_HEADER = ['timestamp', 'test_suite', 'predicted_induced_state',\n",
    "       'three_sec_predicted_induced_state',\n",
    "       'predicted_induced_state_confidence',\n",
    "       'three_sec_predicted_induced_state_confidence', 'top_three_features']\n",
    "\n",
    "param = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'multi:softprob',  # 多分类的问题\n",
    "    'num_class': 6,               # 类别数，与 multisoftmax 并用\n",
    "    'gamma': 0.1,                  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。\n",
    "    'max_depth': 12,               # 构建树的深度，越大越容易过拟合\n",
    "    'lambda': 2,                   # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    'subsample': 0.7,              # 随机采样训练样本\n",
    "    'colsample_bytree': 0.7,       # 生成树时进行的列采样\n",
    "    'min_child_weight': 3,\n",
    "#     'silent': 1,                   # 设置成1则没有运行信息输出，最好是设置为0.\n",
    "    'eta': 0.007,                  # 如同学习率\n",
    "    'seed': 1000,\n",
    "#     'nthread': 4,                  # cpu 线程数\n",
    "}\n",
    "\n",
    "# number of iteration for model training\n",
    "NUM_ROUND = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper (separate file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_time(time_serie):\n",
    "    time_serie = pd.to_numeric(time_serie)\n",
    "    time_serie = time_serie.apply(lambda x: datetime.datetime.fromtimestamp(x/1000000))\n",
    "    time_serie = time_serie.dt.round('1s')\n",
    "    time_serie = time_serie.apply(lambda x: int(datetime.datetime.timestamp(x)*1000000))\n",
    "#     time_serie = time_serie.drop_duplicates()\n",
    "    \n",
    "    return time_serie\n",
    "\n",
    "# return the mode(most comm) element in a series\n",
    "def mode(series):\n",
    "    return series.value_counts().index[0]\n",
    "\n",
    "# return the mean value excluding -9999.9 the default value, if there's normal values\n",
    "def normal_mean(series):\n",
    "    if series.nunique() > 1:\n",
    "        return series[series > -9999.9].mean()\n",
    "    return series.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data path\n",
    "TRAIN_PATH = 'data/data_training.csv'\n",
    "# MODEL PATHS\n",
    "MODEL_PATH = 'submissions/output/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(df, full_df):\n",
    "    # map  'induced_state' for training data\n",
    "    if('induced_state' in df.columns):\n",
    "        df[\"induced_state\"] = df[\"induced_state\"].replace(CATEGORY)\n",
    "    \n",
    "    # round time to whole seconds\n",
    "    df['time'] = round_time(df['time'])    \n",
    "\n",
    "    # aggregate the data by 'time' & 'test_suite' \n",
    "    col_merger = dict(zip(df.columns[2:], [normal_mean]*len(df.columns[2:])))\n",
    "    if('induced_state' in col_merger):\n",
    "        col_merger['induced_state'] = mode\n",
    "    df = df.groupby(['time', 'test_suite']).agg(col_merger)\n",
    "    \n",
    "    return full_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_in_chunks(path, chunksize=500000): \n",
    "    \n",
    "    df = pd.read_csv(path, chunksize=chunksize)\n",
    "    \n",
    "    # read the header cols\n",
    "    with open(path) as f:\n",
    "        header = f.readline().strip().split(',')\n",
    "    \n",
    "    # iterate thru the data in chunks and process them\n",
    "    full_df = pd.DataFrame(columns=header).set_index(['time', 'test_suite'])\n",
    "    for i, c in enumerate(df):\n",
    "        full_df = data_preprocess(c, full_df)\n",
    "\n",
    "        print(f'{(i+1)*chunksize} done....')\n",
    "        \n",
    "    # futher aggregate the duplicates generated from different chunks\n",
    "    col_merger = dict(zip(full_df.columns, [normal_mean]*len(full_df.columns)))\n",
    "    if('induced_state' in col_merger):\n",
    "        col_merger['induced_state'] = mode\n",
    "    full_df = full_df.groupby(['time', 'test_suite']).agg(col_merger)\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess(df, test_size=0):\n",
    "    \n",
    "    # enforce dtypes\n",
    "    df['induced_state'] = df['induced_state'].astype(int)\n",
    "    df['tlx_score'] = df['tlx_score'].astype(int)\n",
    "    \n",
    "    drop_cols = ['induced_state'] #, 'tlx_score']\n",
    "    X = df.loc[:, [ c not in drop_cols for c in df.columns]]\n",
    "    Y = df['induced_state']\n",
    "    \n",
    "    xg_train = xgb.DMatrix(X, label=Y)\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=2)\n",
    "\n",
    "#     xg_train = xgb.DMatrix(X_train, label=y_train)\n",
    "#     xg_test = xgb.DMatrix( X_test, label=y_test)\n",
    "    \n",
    "#     return xg_train, xg_test, X_train, X_test, y_train, y_test\n",
    "\n",
    "    return xg_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000 done....\n",
      "[14:34:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-mlogloss:1.77387\n",
      "[1]\ttrain-mlogloss:1.75660\n",
      "[2]\ttrain-mlogloss:1.74001\n",
      "[3]\ttrain-mlogloss:1.72325\n",
      "[4]\ttrain-mlogloss:1.70623\n",
      "[5]\ttrain-mlogloss:1.68996\n",
      "[6]\ttrain-mlogloss:1.67374\n",
      "[7]\ttrain-mlogloss:1.65792\n",
      "[8]\ttrain-mlogloss:1.64252\n",
      "[9]\ttrain-mlogloss:1.62756\n",
      "[10]\ttrain-mlogloss:1.61260\n",
      "[11]\ttrain-mlogloss:1.59823\n",
      "[12]\ttrain-mlogloss:1.58361\n",
      "[13]\ttrain-mlogloss:1.56942\n",
      "[14]\ttrain-mlogloss:1.55513\n",
      "[15]\ttrain-mlogloss:1.54106\n",
      "[16]\ttrain-mlogloss:1.52737\n",
      "[17]\ttrain-mlogloss:1.51398\n",
      "[18]\ttrain-mlogloss:1.50053\n",
      "[19]\ttrain-mlogloss:1.48749\n",
      "[20]\ttrain-mlogloss:1.47457\n",
      "[21]\ttrain-mlogloss:1.46151\n",
      "[22]\ttrain-mlogloss:1.44900\n",
      "[23]\ttrain-mlogloss:1.43626\n",
      "[24]\ttrain-mlogloss:1.42448\n",
      "[25]\ttrain-mlogloss:1.41256\n",
      "[26]\ttrain-mlogloss:1.40056\n",
      "[27]\ttrain-mlogloss:1.38883\n",
      "[28]\ttrain-mlogloss:1.37708\n",
      "[29]\ttrain-mlogloss:1.36566\n",
      "[30]\ttrain-mlogloss:1.35397\n",
      "[31]\ttrain-mlogloss:1.34238\n",
      "[32]\ttrain-mlogloss:1.33092\n",
      "[33]\ttrain-mlogloss:1.31973\n",
      "[34]\ttrain-mlogloss:1.30904\n",
      "[35]\ttrain-mlogloss:1.29835\n",
      "[36]\ttrain-mlogloss:1.28784\n",
      "[37]\ttrain-mlogloss:1.27698\n",
      "[38]\ttrain-mlogloss:1.26683\n",
      "[39]\ttrain-mlogloss:1.25653\n",
      "[40]\ttrain-mlogloss:1.24634\n",
      "[41]\ttrain-mlogloss:1.23626\n",
      "[42]\ttrain-mlogloss:1.22627\n",
      "[43]\ttrain-mlogloss:1.21666\n",
      "[44]\ttrain-mlogloss:1.20683\n",
      "[45]\ttrain-mlogloss:1.19717\n",
      "[46]\ttrain-mlogloss:1.18740\n",
      "[47]\ttrain-mlogloss:1.17818\n",
      "[48]\ttrain-mlogloss:1.16889\n",
      "[49]\ttrain-mlogloss:1.15979\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    if len(sys.argv) < 2 or len(sys.argv[1]) == 0:\n",
    "        print(\"Training input file is missing.\")\n",
    "        return 1\n",
    "    \n",
    "    if len(sys.argv) < 3 or len(sys.argv[2]) == 0:\n",
    "        print(\"Training output file is missing.\")\n",
    "        return 1\n",
    "    \n",
    "    print('Training started.')\n",
    "    \n",
    "    input_file = sys.argv[1]\n",
    "    model_file = sys.argv[2]\n",
    "    \n",
    "    # My Code Starts HERE\n",
    "    \n",
    "    # load & process data\n",
    "    df = load_data_in_chunks(input_file)\n",
    "    xg_train = train_preprocess(df)\n",
    "    \n",
    "    # train xgb model\n",
    "    watchlist = [ (xg_train,'train')]\n",
    "    num_round = NUM_ROUND\n",
    "    bst = xgb.train(param, xg_train, num_round, watchlist, early_stopping_rounds=10)\n",
    "    \n",
    "    # output trained model\n",
    "    bst.save_model(model_file)\n",
    "\n",
    "    print('Training finished.')\n",
    "\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
